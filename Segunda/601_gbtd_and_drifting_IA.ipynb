{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vhhu79HVkwb5"
      },
      "outputs": [],
      "source": [
        "%pip install scikit-learn==1.3.2\n",
        "%pip install seaborn==0.13.1\n",
        "%pip install numpy==1.26.4\n",
        "%pip install matplotlib==3.7.1\n",
        "%pip install pandas==2.1.4\n",
        "%pip install lightgbm==4.4.0\n",
        "%pip install optuna==3.6.1\n",
        "%pip install python-dotenv\n",
        "%pip install plotly\n",
        "%pip install ipython\n",
        "%pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Cj-rL6xHlA2u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import optuna\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_slice, plot_contour\n",
        "\n",
        "from time import time\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import pickle\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qUQ8mB40Ipl"
      },
      "outputs": [],
      "source": [
        "datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jGKjoN1lRho"
      },
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "\n",
        "# Accedo a variables de entorno\n",
        "dataset_path = os.getenv('DATASET_PATH')\n",
        "dataset_file = os.getenv('DATASET_FILE')\n",
        "mes_test = int(os.getenv('MES_TEST'))\n",
        "trials = int(os.getenv('TRIALS'))\n",
        "\n",
        "ganancia_acierto = 273000\n",
        "costo_estimulo = 7000\n",
        "semillas = [945787,945799,945809,945811,945817]\n",
        "\n",
        "data = pd.read_csv(dataset_path + dataset_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOHKa8QK0Ipm"
      },
      "outputs": [],
      "source": [
        "datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yxw_Iv470Ipm"
      },
      "outputs": [],
      "source": [
        "data.drop([\n",
        "    'Unnamed: 0'\n",
        "], axis=1, inplace=True, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLmHmao30Ipn"
      },
      "outputs": [],
      "source": [
        "data['mpayroll_sobre_edad'] = data['mpayroll'] / data['cliente_edad']\n",
        "\n",
        "# Variables de sumas\n",
        "data['vm_mfinanciacion_limite'] = data[['Master_mfinanciacion_limite', 'Visa_mfinanciacion_limite']].sum(axis=1, skipna=True)\n",
        "data['vm_Fvencimiento'] = data[['Master_Fvencimiento', 'Visa_Fvencimiento']].min(axis=1, skipna=True)\n",
        "data['vm_Finiciomora'] = data[['Master_Finiciomora', 'Visa_Finiciomora']].min(axis=1, skipna=True)\n",
        "data['vm_msaldototal'] = data[['Master_msaldototal', 'Visa_msaldototal']].sum(axis=1, skipna=True)\n",
        "data['vm_msaldopesos'] = data[['Master_msaldopesos', 'Visa_msaldopesos']].sum(axis=1, skipna=True)\n",
        "data['vm_msaldodolares'] = data[['Master_msaldodolares', 'Visa_msaldodolares']].sum(axis=1, skipna=True)\n",
        "data['vm_mconsumospesos'] = data[['Master_mconsumospesos', 'Visa_mconsumospesos']].sum(axis=1, skipna=True)\n",
        "data['vm_mconsumosdolares'] = data[['Master_mconsumosdolares', 'Visa_mconsumosdolares']].sum(axis=1, skipna=True)\n",
        "data['vm_mlimitecompra'] = data[['Master_mlimitecompra', 'Visa_mlimitecompra']].sum(axis=1, skipna=True)\n",
        "data['vm_madelantopesos'] = data[['Master_madelantopesos', 'Visa_madelantopesos']].sum(axis=1, skipna=True)\n",
        "data['vm_madelantodolares'] = data[['Master_madelantodolares', 'Visa_madelantodolares']].sum(axis=1, skipna=True)\n",
        "data['vm_fultimo_cierre'] = data[['Master_fultimo_cierre', 'Visa_fultimo_cierre']].max(axis=1, skipna=True)\n",
        "data['vm_mpagado'] = data[['Master_mpagado', 'Visa_mpagado']].sum(axis=1, skipna=True)\n",
        "data['vm_mpagospesos'] = data[['Master_mpagospesos', 'Visa_mpagospesos']].sum(axis=1, skipna=True)\n",
        "data['vm_mpagosdolares'] = data[['Master_mpagosdolares', 'Visa_mpagosdolares']].sum(axis=1, skipna=True)\n",
        "data['vm_fechaalta'] = data[['Master_fechaalta', 'Visa_fechaalta']].max(axis=1, skipna=True)\n",
        "data['vm_mconsumototal'] = data[['Master_mconsumototal', 'Visa_mconsumototal']].sum(axis=1, skipna=True)\n",
        "data['vm_cconsumos'] = data[['Master_cconsumos', 'Visa_cconsumos']].sum(axis=1, skipna=True)\n",
        "data['vm_cadelantosefectivo'] = data[['Master_cadelantosefectivo', 'Visa_cadelantosefectivo']].sum(axis=1, skipna=True)\n",
        "data['vm_mpagominimo'] = data[['Master_mpagominimo', 'Visa_mpagominimo']].sum(axis=1, skipna=True)\n",
        "\n",
        "# Variables de ratios\n",
        "data['vmr_Master_mlimitecompra'] = data['Master_mlimitecompra'] / data['vm_mlimitecompra']\n",
        "data['vmr_Visa_mlimitecompra'] = data['Visa_mlimitecompra'] / data['vm_mlimitecompra']\n",
        "data['vmr_msaldototal'] = data['vm_msaldototal'] / data['vm_mlimitecompra']\n",
        "data['vmr_msaldopesos'] = data['vm_msaldopesos'] / data['vm_mlimitecompra']\n",
        "data['vmr_msaldopesos2'] = data['vm_msaldopesos'] / data['vm_msaldototal']\n",
        "data['vmr_msaldodolares'] = data['vm_msaldodolares'] / data['vm_mlimitecompra']\n",
        "data['vmr_msaldodolares2'] = data['vm_msaldodolares'] / data['vm_msaldototal']\n",
        "data['vmr_mconsumospesos'] = data['vm_mconsumospesos'] / data['vm_mlimitecompra']\n",
        "data['vmr_mconsumosdolares'] = data['vm_mconsumosdolares'] / data['vm_mlimitecompra']\n",
        "data['vmr_madelantopesos'] = data['vm_madelantopesos'] / data['vm_mlimitecompra']\n",
        "data['vmr_madelantodolares'] = data['vm_madelantodolares'] / data['vm_mlimitecompra']\n",
        "data['vmr_mpagado'] = data['vm_mpagado'] / data['vm_mlimitecompra']\n",
        "data['vmr_mpagospesos'] = data['vm_mpagospesos'] / data['vm_mlimitecompra']\n",
        "data['vmr_mpagosdolares'] = data['vm_mpagosdolares'] / data['vm_mlimitecompra']\n",
        "data['vmr_mconsumototal'] = data['vm_mconsumototal'] / data['vm_mlimitecompra']\n",
        "data['vmr_mpagominimo'] = data['vm_mpagominimo'] / data['vm_mlimitecompra']\n",
        "\n",
        "\n",
        "\n",
        "# Filtramos solo las columnas numéricas\n",
        "numeric_cols = data.select_dtypes(include=[np.number])\n",
        "\n",
        "# Reemplazo valores infinitos con NaN solo en las columnas numéricas\n",
        "infinitos_qty = np.isinf(numeric_cols).sum().sum()\n",
        "if infinitos_qty > 0:\n",
        "    print(f\"ATENCIÓN: Hay {infinitos_qty} valores infinitos en tu dataset. Serán pasados a NaN.\")\n",
        "    data[numeric_cols.columns] = numeric_cols.replace([np.inf, -np.inf], np.nan)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpMaIH7i0Ipn"
      },
      "outputs": [],
      "source": [
        "#Feature Eengeniering\n",
        "cols_lagueables = data.columns.difference([\"numero_de_cliente\", \"foto_mes\", \"clase_ternaria\"])\n",
        "\n",
        "# Create lagged columns\n",
        "for lag in [1, 2]:\n",
        "    lagged_df = data.groupby(\"numero_de_cliente\")[cols_lagueables].shift(lag)\n",
        "    lagged_df.columns = [f\"{col}_lag{lag}\" for col in cols_lagueables]\n",
        "    data = pd.concat([data, lagged_df], axis=1)\n",
        "\n",
        "# Add delta columns\n",
        "for col in cols_lagueables:\n",
        "    data[f\"{col}_delta1\"] = data[col] - data[f\"{col}_lag1\"]\n",
        "    data[f\"{col}_delta2\"] = data[col] - data[f\"{col}_lag2\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X19taoF0Ipn"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import median_abs_deviation\n",
        "\n",
        "null_count_by_month = data.groupby('foto_mes').apply(lambda df: df.isnull().sum())\n",
        "# Filtrar columnas con al menos un valor distinto de 0\n",
        "filtered_columns = null_count_by_month.loc[:, (null_count_by_month != 0).any(axis=0)]\n",
        "\n",
        "# Calcular la mediana y la desviación absoluta de la mediana (MAD) por columna\n",
        "median_values = filtered_columns.median()\n",
        "mad_values = filtered_columns.apply(median_abs_deviation)\n",
        "\n",
        "# Identificar columnas donde alguna fila supera 3 veces la MAD\n",
        "columns_with_outliers = filtered_columns.loc[:, ((filtered_columns - median_values).abs() > (3 * mad_values)).any(axis=0)]\n",
        "\n",
        "\n",
        "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "output_file_cols = dataset_path + \"resultados_columnas_nan\" + now + \".csv\"\n",
        "\n",
        "columns_with_outliers.to_csv(output_file_cols, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uSAeevr0Ipo"
      },
      "outputs": [],
      "source": [
        "# Assuming 'data' is your DataFrame and 'dataset_path' is defined\n",
        "null_count_by_month = data.groupby('foto_mes').apply(lambda df: df.isnull().sum())\n",
        "\n",
        "# Filter columns where all values are zero across all rows\n",
        "filtered_columns = null_count_by_month.loc[:, (null_count_by_month == 0).all(axis=0)]\n",
        "\n",
        "# Calculate median and median absolute deviation (MAD) for the remaining columns\n",
        "median_values = filtered_columns.median()\n",
        "mad_values = filtered_columns.apply(median_abs_deviation)\n",
        "\n",
        "# Identify columns where any row exceeds 3 times the MAD\n",
        "columns_with_outliers = filtered_columns.loc[:, ((filtered_columns - median_values).abs() > (3 * mad_values)).any(axis=0)]\n",
        "\n",
        "# Export to a CSV file\n",
        "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "output_file_cols = dataset_path + \"resultados_columnas_ceros\" + now + \".csv\"\n",
        "columns_with_outliers.to_csv(output_file_cols, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tudKPm5e0Ipo"
      },
      "outputs": [],
      "source": [
        "#ELIMINO MESES ROTOS\n",
        "\n",
        "# Define the values you want to exclude\n",
        "values_to_drop = [202006, 201905, 201910]\n",
        "\n",
        "# Drop the rows where 'foto_mes' is in the list of values\n",
        "data = data[~data['foto_mes'].isin(values_to_drop)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2awsEXwO5_w"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlYeDIBQP3-s"
      },
      "outputs": [],
      "source": [
        "data['clase_peso'] = 1.0\n",
        "\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
        "data.loc[data['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV1meQ5cZ_Sl"
      },
      "outputs": [],
      "source": [
        "data['clase_binaria1'] = np.where(data['clase_ternaria'] == 'BAJA+2', 1, 0)\n",
        "data['clase_binaria2'] = np.where(data['clase_ternaria'] == 'CONTINUA', 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTcxuFejUeCl"
      },
      "outputs": [],
      "source": [
        "valores_unicos = data['clase_ternaria'].unique()\n",
        "print(valores_unicos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVDW-YPNXkXj"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4HcXZUt0Ipp"
      },
      "outputs": [],
      "source": [
        "# Defino dataset de train y dataset de test\n",
        "\n",
        "train_data = data[(data['foto_mes'] != mes_test) & (data['foto_mes'] != mes_test - 1)]\n",
        "#train_data = data[data['foto_mes'].isin([202101, 202102, 202103])]\n",
        "\n",
        "test_data = data[data['foto_mes'] == mes_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ8Qm0vqvF_F"
      },
      "outputs": [],
      "source": [
        "X_train = train_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_train_binaria1 = train_data['clase_binaria1']\n",
        "y_train_binaria2 = train_data['clase_binaria2']\n",
        "w_train = train_data['clase_peso']\n",
        "\n",
        "X_test = test_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
        "y_test_binaria1 = test_data['clase_binaria1']\n",
        "y_test_class = test_data['clase_ternaria']\n",
        "w_test = test_data['clase_peso']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cqbDiI4x2OD"
      },
      "outputs": [],
      "source": [
        "#imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "#Xif = imp_mean.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FvDUXeatl66"
      },
      "outputs": [],
      "source": [
        "def lgb_gan_eval(y_pred, data):\n",
        "    weight = data.get_weight()\n",
        "    ganancia = np.where(weight == 1.00002, ganancia_acierto, 0) - np.where(weight < 1.00002, costo_estimulo, 0)\n",
        "    ganancia = ganancia[np.argsort(y_pred)[::-1]]\n",
        "    ganancia = np.cumsum(ganancia)\n",
        "\n",
        "    return 'gan_eval', np.max(ganancia) , True\n",
        "\n",
        "# Parámetros del modelo\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'gan_eval',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'max_bin': 31,\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.01,\n",
        "    'feature_fraction': 0.3,\n",
        "    'bagging_fraction': 0.7,\n",
        "    'verbose': 0\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dnWsRWgPnRr"
      },
      "outputs": [],
      "source": [
        "train_data1 = lgb.Dataset(X_train, label=y_train_binaria1, weight=w_train)\n",
        "train_data2 = lgb.Dataset(X_train, label=y_train_binaria2, weight=w_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibXn1tiLNT6J"
      },
      "outputs": [],
      "source": [
        "cv_results1 = lgb.cv(\n",
        "    params,\n",
        "    train_data1,\n",
        "    num_boost_round=150,\n",
        "    feval=lgb_gan_eval,\n",
        "    nfold=5,\n",
        "    seed=semillas[0]\n",
        ")\n",
        "\n",
        "cv_results2 = lgb.cv(\n",
        "    params,\n",
        "    train_data2,\n",
        "    num_boost_round=150,\n",
        "    feval=lgb_gan_eval,\n",
        "    nfold=5,\n",
        "    seed=semillas[0]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vJohBQq0Ipr"
      },
      "outputs": [],
      "source": [
        "datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOoWx9sbhx5h"
      },
      "outputs": [],
      "source": [
        "df_ganancias = pd.DataFrame({\n",
        "    'binaria1': cv_results1['valid gan_eval-mean'],\n",
        "    'binaria2': cv_results2['valid gan_eval-mean'],\n",
        "    'Iteracion': range(1, len(cv_results1['valid gan_eval-mean']) + 1)\n",
        "})\n",
        "\n",
        "# Normalizamos la ganancias\n",
        "df_ganancias['binaria1'] = df_ganancias['binaria1']*5\n",
        "df_ganancias['binaria2'] = df_ganancias['binaria2']*5\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x='Iteracion', y='binaria1', data=df_ganancias, label='binaria 1')\n",
        "sns.lineplot(x='Iteracion', y='binaria2', data=df_ganancias, label='binaria 2')\n",
        "plt.title('Comparación de las Ganancias de las 2 clases binarias')\n",
        "plt.xlabel('Iteración')\n",
        "plt.ylabel('Ganancia')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYMEnNFbkSoQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def objective(trial):\n",
        "    num_leaves = trial.suggest_int('num_leaves', 8, 100),\n",
        "    learning_rate = trial.suggest_float('learning_rate', 0.005, 0.3), # mas bajo, más iteraciones necesita\n",
        "    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 1, 1000),\n",
        "    feature_fraction = trial.suggest_float('feature_fraction', 0.1, 1.0),\n",
        "    bagging_fraction = trial.suggest_float('bagging_fraction', 0.1, 1.0),\n",
        "\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'custom',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'first_metric_only': True,\n",
        "        'boost_from_average': True,\n",
        "        'feature_pre_filter': False,\n",
        "        'max_bin': 31,\n",
        "        'num_leaves': num_leaves,\n",
        "        'learning_rate': learning_rate,\n",
        "        'min_data_in_leaf': min_data_in_leaf,\n",
        "        'feature_fraction': feature_fraction,\n",
        "        'bagging_fraction': bagging_fraction,\n",
        "        'seed': semillas[0],\n",
        "        'verbose': -1\n",
        "    }\n",
        "\n",
        "    train_data = lgb.Dataset(X_train,\n",
        "                              label=y_train_binaria2, # eligir la clase\n",
        "                              weight=w_train)\n",
        "    cv_results = lgb.cv(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=100, # modificar, subit y subir... y descomentar la línea inferior\n",
        "        # early_stopping_rounds= int(50 + 5 / learning_rate),\n",
        "        feval=lgb_gan_eval,\n",
        "        stratified=True,\n",
        "        nfold=5,\n",
        "        seed=semillas[0]\n",
        "    )\n",
        "    max_gan = max(cv_results['valid gan_eval-mean'])\n",
        "    best_iter = cv_results['valid gan_eval-mean'].index(max_gan) + 1\n",
        "\n",
        "    # Guardamos cual es la mejor iteración del modelo\n",
        "    trial.set_user_attr(\"best_iter\", best_iter)\n",
        "\n",
        "    return max_gan * 5\n",
        "\n",
        "\n",
        "# now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") # Variable para usar una db ya existente\n",
        "\n",
        "storage_name = \"sqlite:///\" + dataset_path + \"optimization_lgbm\" + now + \".db\"\n",
        "study_name = \"exp_301_lgbm\"\n",
        "\n",
        "print('storage_name', storage_name)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    study_name=study_name,\n",
        "    storage=storage_name,\n",
        "    load_if_exists=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DI7zXUryMw4"
      },
      "outputs": [],
      "source": [
        "new_var = study.optimize(objective, n_trials=trials) # Ajustar a 500 en gcloud\n",
        "new_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CcHoUUU0Ipr"
      },
      "outputs": [],
      "source": [
        "datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fH4ybQgYx7Xf"
      },
      "outputs": [],
      "source": [
        "optuna.visualization.plot_optimization_history(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOfm5DXAx8Rj"
      },
      "outputs": [],
      "source": [
        "plot_param_importances(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0U6CfznSx-gG"
      },
      "outputs": [],
      "source": [
        "plot_slice(study)\n",
        "\n",
        "plot_contour(study, params=['num_leaves','min_data_in_leaf'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bwyUriQksZAM"
      },
      "outputs": [],
      "source": [
        "best_iter = study.best_trial.user_attrs[\"best_iter\"]\n",
        "print(f\"Mejor cantidad de árboles para el mejor model {best_iter}\")\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'first_metric_only': True,\n",
        "    'boost_from_average': True,\n",
        "    'feature_pre_filter': False,\n",
        "    'max_bin': 31,\n",
        "    'num_leaves': study.best_trial.params['num_leaves'],\n",
        "    'learning_rate': study.best_trial.params['learning_rate'],\n",
        "    'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],\n",
        "    'feature_fraction': study.best_trial.params['feature_fraction'],\n",
        "    'bagging_fraction': study.best_trial.params['bagging_fraction'],\n",
        "    'seed': semillas[0],\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "train_data = lgb.Dataset(X_train,\n",
        "                          label=y_train_binaria2,\n",
        "                          weight=w_train)\n",
        "\n",
        "model = lgb.train(params,\n",
        "                  train_data,\n",
        "                  num_boost_round=best_iter)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l7ZObpkHtnUl"
      },
      "outputs": [],
      "source": [
        "importances = model.feature_importance()\n",
        "feature_names = X_train.columns.tolist()\n",
        "importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "importance_df = importance_df.sort_values('importance', ascending=False)\n",
        "importance_df[importance_df['importance'] > 0].to_string()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCKqNU8Q0Ipv"
      },
      "outputs": [],
      "source": [
        "importance_df[importance_df['importance'] == 0].to_string()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W5IWjUv0Ipv"
      },
      "outputs": [],
      "source": [
        "# Opcional: guardo modelo como txt\n",
        "\n",
        "# model.save_model(modelos_path + 'lgb_first.txt')\n",
        "# model = lgb.Booster(model_file=modelos_path + 'lgb_first.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Bac1TlkZjuqE"
      },
      "outputs": [],
      "source": [
        "y_pred_lgm = model.predict(X_test)\n",
        "y_pred_lgm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iKcXUhCHlywR"
      },
      "outputs": [],
      "source": [
        "# Supongamos que 'X_test' es tu DataFrame original del que deseas conservar el resto\n",
        "y_pred_prob = model.predict(X_test)\n",
        "\n",
        "# Convertir a predicciones binarias usando un umbral de 0.025\n",
        "threshold = 0.025\n",
        "# threshold = 0.01\n",
        "\n",
        "#probar cambiando el umbral\n",
        "y_pred_binary = (y_pred_prob >= threshold).astype(int)\n",
        "\n",
        "# Agregar las columnas de probabilidades y predicciones al DataFrame original\n",
        "X_test['probabilidad'] = y_pred_prob\n",
        "X_test['prediccion'] = y_pred_binary\n",
        "\n",
        "X_test.prediccion.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6llnHQxNoTEF"
      },
      "outputs": [],
      "source": [
        "# Filtrar el DataFrame para quedarte solo con 'numero_de_cliente' y 'prediccion'\n",
        "result_df = X_test[['numero_de_cliente', 'prediccion']]\n",
        "\n",
        "# Renombrar la columna 'prediccion' a 'Predicted' si es necesario\n",
        "result_df.rename(columns={'prediccion': 'Predicted'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HFD46lIRpa0F"
      },
      "outputs": [],
      "source": [
        "# Especificar la ruta completa del archivo donde deseas guardar el DataFrame\n",
        "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "output_file = dataset_path + \"resultados_predicciones\" + now + \".csv\"\n",
        "\n",
        "print('output_file', output_file)\n",
        "\n",
        "# Guardar el DataFrame como un archivo CSV en la ruta especificada\n",
        "result_df.to_csv(output_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OMMfjjW0Ipw"
      },
      "outputs": [],
      "source": [
        "datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}